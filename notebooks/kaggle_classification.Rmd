---
title: "Classification on Brazilian elections"
author: "José Benardi de Souza Nunes"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---


```{r}
library(dataPreparation)
library(tidyverse)
library(MLmetrics)
library(ggmosaic)
library(magrittr)
library(GGally)
library(rattle)
library(caret)
library(here)
library(DMwR)

theme_set(theme_bw())
```

# Data Overview

## Loading Data

```{r}
readr::read_csv(here::here('data/train_class.csv'),
                progress = FALSE,
                local=readr::locale("br"),
                col_types = cols(ano = col_integer(),
                                 sequencial_candidato = col_character(),
                                 quantidade_doacoes = col_integer(),
                                 quantidade_doadores = col_integer(),
                                 total_receita = col_double(),
                                 media_receita = col_double(),
                                 recursos_de_outros_candidatos.comites = col_double(),
                                 recursos_de_pessoas_fisicas = col_double(),
                                 recursos_de_pessoas_juridicas = col_double(),
                                 recursos_proprios = col_double(),
                                 `recursos_de_partido_politico` = col_double(),
                                 quantidade_despesas = col_integer(),
                                 quantidade_fornecedores = col_integer(),
                                 total_despesa = col_double(),
                                 media_despesa = col_double(),
                                 situacao = col_character(),
                                 .default = col_character())) %>%
  mutate(sequencial_candidato = as.numeric(sequencial_candidato),
         estado_civil = as.factor(estado_civil),
         ocupacao = as.factor(ocupacao),
         situacao = as.factor(situacao),
         partido = as.factor(partido),
         grau = as.factor(grau),
         sexo = as.factor(sexo),
         uf = as.factor(uf)) -> data

data %>%
  glimpse()
```

```{r}
data %>%
  map_df(function(x) sum(is.na(x))) %>%
  gather(feature, num_nulls) %>%
  arrange(desc(num_nulls))
```

## Data Exploration

### Imbalance on class distribution

```{r}
data %>%
  ggplot(aes(situacao)) +
  geom_bar() +
  labs(x="Situation", y="Absolute Frequency")
```

```{r}
data %>%
  group_by(situacao) %>%
  summarise(num = n()) %>%
  ungroup() %>%
  mutate(total = sum(num),
         proportion = num/total)
```


#### There's a strong imbalance in the class distribution of the dataset

* This imbalance can lead to a bias in the model that will learn to overlook the less frequent classes. Such bias can have a negative impact in the model generalization and its performance.
    + We can restore balance by removing instances from the most frequent class $undersampling$.
    + We can restore balance by adding instances from the most frequent class $oversampling$.

```{r}
data %>% 
  select(-ano,
         -sequencial_candidato,
         -nome) %>%
  select(
    quantidade_doacoes,
    quantidade_doadores,
    total_receita,
    media_receita,
    recursos_de_outros_candidatos.comites,
    recursos_de_pessoas_fisicas,
    recursos_de_pessoas_juridicas,
    recursos_proprios,
    `recursos_de_partido_politico`) %>%
  na.omit() %>%
  ggcorr(palette = "RdBu", label = TRUE,
       hjust = 0.95, label_size = 3,size = 3,
       nbreaks = 5, layout.exp = 5) +
  ggtitle("Correlation plot for employed variables")
```

```{r}
data %>%
  ggplot(aes(situacao,recursos_proprios)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from personal resources (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_partido_politico)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from political party. (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_outros_candidatos.comites)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from other candidate’s committees (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_fisicas)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from individuals (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_juridicas)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from legal entities (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             quantidade_doacoes)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Number of donations", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             quantidade_doadores)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Number of donators", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             media_receita)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Mean expenditure", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             total_receita)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Total expenditure", x="Situation")
```

```{r}
data %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  guides(fill = guide_legend(title = "Sex"))  +
  labs(x="Situation") 
```

```{r}
data %>%
ggplot() +
  geom_mosaic(aes(x = product(grau, situacao),
                   fill=grau)) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  guides(fill = guide_legend(title = "Level of education"))  +
  labs(x="Situation") 
```

```{r}
data %>%
ggplot() +
   geom_mosaic(aes(x = product(estado_civil, situacao),
                   fill=estado_civil)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  guides(fill = guide_legend(title = "Marital Status"))  +
  labs(x="Situation") 
```

### Splitting data and Encoding

```{r}
set.seed(107)

data$id <- 1:nrow(data)

data %>% 
  dplyr::sample_frac(.8) -> train

cat("#### Train Shape",
    "\n##### Observations: ",nrow(train),
    "\n##### Variables: ",ncol(train))
```

```{r}
dplyr::anti_join(data, 
                 train, 
                 by = 'id') -> test

cat("#### Test Shape",
    "\n##### Observations: ",nrow(test),
    "\n##### Variables: ",ncol(test))
```

```{r results='asis'}
encoding <- build_encoding(dataSet = data,
                          cols = c("uf","sexo","grau","ocupacao",
                                   "partido","estado_civil"),
                          verbose = F)

train <- one_hot_encoder(dataSet = train,
                          encoding = encoding,
                          drop = TRUE,
                          verbose = F)

cat("#### Train Shape",
    "\n##### Observations: ",nrow(train),
    "\n##### Variables: ",ncol(train))
```

```{r}
test <- one_hot_encoder(dataSet = test,
                          encoding = encoding,
                          drop = TRUE,
                          verbose = F)

cat("#### Data Shape",
    "\n##### Observations: ",nrow(test),
    "\n##### Variables: ",ncol(test))
```

## Near Zero Variance Predictors

```{r}
train %>%
  nearZeroVar(saveMetrics = TRUE) %>%
  tibble::rownames_to_column("variable") %>%
  filter(nzv == T) %>% 
  pull(variable) -> near_zero_vars

train %>% 
    select(-one_of(near_zero_vars)) %>%
    select(-ano,-nome,-id,-sequencial_candidato) -> train

test %>%
    select(-one_of(near_zero_vars)) %>%
    select(-ano,-nome,-id,-sequencial_candidato) -> test


near_zero_vars %>% 
  glimpse() 
```

## Scale and Center

```{r}
train %>%
  preProcess(.,method = c("center","scale")) -> processParams

processParams %>%
  predict(.,train) -> train 

processParams %>% 
  predict(.,test) -> test 

processParams
```

# Logistic Regression 

```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred,
                     y_true = data$obs,
                     positive = lev[1])
  c(F1 = f1_val)
}

F_Measure <- function(expected, predicted, ...) {
  data.frame(expected=expected,
             prediction=predicted) %>%
      mutate(TP = ifelse(expected == "eleito" & 
                         prediction == "eleito",1,0),
             TN = ifelse(expected == "nao_eleito" &
                         prediction == "nao_eleito",1,0),
             FN = ifelse(expected == "eleito" &
                         prediction == "nao_eleito",1,0),
             FP = ifelse(expected == "nao_eleito" &
                         prediction == "eleito",1,0)) -> result
  result  %>%
    summarize(TP = sum(TP),
              TN = sum(TN),
              FP = sum(FP),
              FN = sum(FN)) %>%
    mutate(recall = TP / (TP + FN),
           precision = TP / (TP + FP),
           accuracy = (TP + TN)/(TP + TN + FP + FN),
           f_measure = 2 * (precision * recall) / (precision + recall)) -> result
  
  return(result)
}
```


```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = c(0.001,0.01) )
train %>%
  caret::train(situacao ~ .,
               data= .,
               method = "regLogistic",
               metric = "F1",
               trControl = trainControl(method = "boot",
                                        classProbs = TRUE,
                                        summaryFunction = f1,
                                        savePredictions = "final"),
               tuneGrid = rlGrid) -> model.rl

model.rl
```

```{r}
model.rl %$%
  results %>%
  mutate(cost=as.factor(cost)) %>%
  ggplot(aes(epsilon,F1,
             color=cost)) +
  geom_line() +
  geom_point() +
  labs(y= "F1 (Bootstrap)", x="Tolerance") +
  facet_wrap(. ~ loss, labeller = "label_both") +
  guides(color = guide_legend(title = "Cost")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

```{r}
model.rl %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

## Quality metric

### Train and Validation

```{r}
model.rl %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.rl,.) %>%
  F_Measure(test$situacao,.)
```

# K nearest neighbours

```{r}
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

train %>%
  train(situacao ~ .,
        data = .,
        metric = "F1",
        method = "knn",
        na.action = na.omit,
        tuneGrid = neighborsGrid,
        trControl = trainControl(method = "boot",
                                 classProbs = TRUE,
                                 summaryFunction = f1,
                                 savePredictions = "final")) -> model.knn

model.knn
```

```{r}
model.knn %$%
  bestTune %$% 
  k -> bestParameter

model.knn %$%
  results %>%
  ggplot(aes(k,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="#Neighbors",
       y="F1 (Bootstrap)") 
```

```{r}
model.knn %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```


## Quality metric

### Train and Validation

```{r}
model.knn %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.knn,.) %>%
  F_Measure(test$situacao,.)
```

# Decision Tree

```{r}
rpart.grid <- expand.grid(.cp = seq(from=0, to=0.1, by=0.005))

caret::train(x = select(train, -situacao),
             y = train$situacao,
             metric = "F1",
             method = "rpart",
             na.action = na.omit,
             tuneGrid = rpart.grid,
             trControl = trainControl(method = "boot",
                                      classProbs = TRUE,
                                      summaryFunction = f1,
                                      savePredictions = "final")) -> model.tree

model.tree
```

```{r}
model.tree %$%
  bestTune %$% 
  cp -> bestParameter

model.tree %$%
  results %>%
  ggplot(aes(cp,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="Complexity Parameter",
       y="F1 (Bootstrap)") 
```

```{r}
model.tree %$%
  finalModel %>%
  fancyRpartPlot(sub="")
```

```{r}
model.tree %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,Overall),y = Overall),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```


## Quality metric

### Train and Validation

```{r}
model.tree %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.tree,.) %>%
  F_Measure(test$situacao,.)
```

# AdaBoost

```{r}
gbmGrid <- expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                       shrinkage = c(.12, .13),
                       n.trees = c(55, 56, 57, 58, 59, 60),
                       n.minobsinnode = c(2,3))

seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

train %>%
  train(situacao ~ .,
        data = ., 
        method = "gbm", 
        metric = "F1", 
        verbose = FALSE,
        tuneGrid = gbmGrid,
        distribution = "adaboost",
        trControl = trainControl(savePredictions = "final",
                                      summaryFunction = f1,
                                      classProbs = TRUE,
                                      method = "boot",
                                      seeds = seeds)) -> model.gbm
model.gbm
```

```{r}
model.gbm %$%
  results %>%
  ggplot(aes(n.trees,F1, 
             color=as.factor(interaction.depth))) +
  geom_point() +
  geom_line() +
  facet_wrap(. ~ n.minobsinnode+ shrinkage,
             labeller = "label_both") +
  labs(x="# Boosting Iterations",y="F1 (Bootstrap)") +
  guides(color = guide_legend(title = "Max Tree Depth")) 
```

### Train and Validation

```{r}
model.gbm %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.gbm,.) %>%
  F_Measure(test$situacao,.)
```

# Applying Oversample (SMOTE)

```{r}
train %>%
  SMOTE(situacao ~ .,
        data = ., 
        perc.over = 200, 
        perc.under=200) -> oversampled

cat("#### Train Shape",
    "\n##### Observations: ",nrow(oversampled),
    "\n##### Variables: ",ncol(oversampled))
```


```{r}
oversampled %>%
  group_by(situacao) %>%
  summarise(num = n()) %>%
  ungroup() %>%
  mutate(total = sum(num),
         proportion = num/total)
```

# Logistic Regression with SMOTE 

```{r}
rlGrid <- expand.grid( cost = c(0.02,0.1,2,20,100,200),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = seq(from=0.001,to=0.1, by=0.005) )
oversampled %>%
  caret::train(situacao ~ .,
               data= .,
               method = "regLogistic",
               metric = "F1",
               trControl = trainControl(method = "boot",
                                        classProbs = TRUE,
                                        summaryFunction = f1,
                                        savePredictions = "final"),
               tuneGrid = rlGrid) -> model.rl.smote

model.rl.smote
```

```{r}
plot(model.rl.smote)
```


```{r}
model.rl.smote %$%
  results %>%
  mutate(cost=as.factor(cost)) %>%
  ggplot(aes(epsilon,F1,
             color=cost)) +
  geom_line() +
  geom_point() +
  labs(y= "F1 (Bootstrap)", x="Tolerance") +
  facet_wrap(. ~ loss, labeller = "label_both") +
  guides(color = guide_legend(title = "Cost")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

```{r}
model.rl.smote %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

## Quality metric

### Train and Validation

```{r}
model.rl.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.rl.smote,.) %>%
  F_Measure(test$situacao,.)
```

# K nearest neighbours with Smote

```{r}
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

oversampled %>%
  train(situacao ~ .,
        data = .,
        metric = "F1",
        method = "knn",
        na.action = na.omit,
        tuneGrid = neighborsGrid,
        trControl = trainControl(method = "boot",
                                 classProbs = TRUE,
                                 summaryFunction = f1,
                                 savePredictions = "final")) -> model.knn.smote

model.knn.smote
```

```{r}
model.knn.smote %$%
  bestTune %$% 
  k -> bestParameter

model.knn.smote %$%
  results %>%
  ggplot(aes(k,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="#Neighbors",
       y="F1 (Bootstrap)") 
```

```{r}
model.knn.smote %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```


## Quality metric

### Train and Validation

```{r}
model.knn.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.knn.smote,.) %>%
  F_Measure(test$situacao,.)
```

# Decision Tree with SMOTE

```{r}
rpart.grid <- expand.grid(.cp = seq(from=0, to=0.1, by=0.005))

caret::train(x = select(oversampled, -situacao),
             y = oversampled$situacao,
             metric = "F1",
             method = "rpart",
             na.action = na.omit,
             tuneGrid = rpart.grid,
             trControl = trainControl(method = "boot",
                                      classProbs = TRUE,
                                      summaryFunction = f1,
                                      savePredictions = "final")) -> model.tree.smote

model.tree.smote
```

```{r}
model.tree.smote %$%
  bestTune %$% 
  cp -> bestParameter

model.tree.smote %$%
  results %>%
  ggplot(aes(cp,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="Complexity Parameter",
       y="F1 (Bootstrap)") 
```

```{r}
model.tree.smote %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,Overall),y = Overall),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

```{r}
model.tree.smote %$%
  finalModel %>%
  fancyRpartPlot(sub="")
```


## Quality metric

### Train and Validation

```{r}
model.tree.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.tree.smote,.) %>%
  F_Measure(test$situacao,.)
```

# AdaBoost with SMOTE

```{r}
gbmGrid <- expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                       shrinkage = c(.12, .13),
                       n.trees = c(55, 56, 57, 58, 59, 60),
                       n.minobsinnode = c(2,3))

seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

oversampled %>%
  train(situacao ~ .,
        data = ., 
        method = "gbm", 
        metric = "F1", 
        verbose = FALSE,
        tuneGrid = gbmGrid,
        distribution = "adaboost",
        trControl = trainControl(savePredictions = "final",
                                      summaryFunction = f1,
                                      classProbs = TRUE,
                                      method = "boot",
                                      seeds = seeds)) -> model.gbm.smote
model.gbm.smote
```

```{r}
model.gbm.smote %$%
  results %>%
  ggplot(aes(n.trees,F1, 
             color=as.factor(interaction.depth))) +
  geom_point() +
  geom_line() +
  facet_wrap(. ~ n.minobsinnode+ shrinkage,
             labeller = "label_both") +
  labs(x="# Boosting Iterations",y="F1 (Bootstrap)") +
  guides(color = guide_legend(title = "Max Tree Depth")) 
```

### Train and Validation

```{r}
model.gbm.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.gbm.smote,.) %>%
  F_Measure(test$situacao,.)
```
