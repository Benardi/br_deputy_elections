---
title: "Classification of candidates in Brazilian elections"
subtitle: "Binary Classification of candidates in previous Brazilian elections"
author: "José Benardi de Souza Nunes"
date: 01/12/2018
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

<br>

# Introduction

<br>

> Data Analysis and Classification on a subset of data about polls for the 2006 and 2010 elections in Brazil for the "Câmara Federal de Deputados". Data was taken from the [TSE portal](http://www.tse.jus.br/) which originally encompassed approximately 7300 candidates.

<br>

***

<br>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(dataPreparation)
library(tidyverse)
library(MLmetrics)
library(ggmosaic)
library(magrittr)
library(GGally)
library(rattle)
library(caret)
library(here)
library(DMwR)

theme_set(theme_bw())
```

# Data Overview

## The variables 

<br>

```
The response variable is the variable that you are interesting in making measurements and conclusions on.

A predictor variable is a variable used to predict another variable.

Our response variable will be "situacao", we want to study how well the predictor variables can help predict its behavior and how they impact in the linear regression.
```

<br>

#### Each item corresponds to a candidate, the attributes of each item are as follows:

- **ano** : Year at which the election took place.
- **sequencial_candidato** : Sequential ID to map the candidates
- **nome** : Name of the candidate
- **uf** : Federate state to which the candidate belongs.
- **partido** : Political party to which the candidate belongs.
- **quantidade_doacoes** : Number of donations received during political campaign.
- **quantidade_doadores** : Number of donators that contributed to the candidate's political campaign.
- **total_receita** : Total revenue. 
- **media_receita** : Mean revenue. 
- **recursos_de_outros_candidatos.comites** : Revenue from other candidate's committees.
- **recursos_de_pessoas_fisicas** : Revenue from individuals.
- **recursos_de_pessoas_juridicas** : Revenue from legal entities.
- **recursos_proprios** : Revenue from personal resources.
- **recursos_de_partido_politico** : Revenue from political party.
- **quantidade_despesas** : Number of expenses.
- **quantidade_fornecedores** : Number of suppliers.
- **total_despesa** : Total expenditure. 
- **media_despesa** : Mean expenditure.
- **cargo** : Position.
- **sexo** : Sex.
- **grau** : Level of education.
- **estado_civil** : Marital status. 
- **ocupacao** : Candidate's occupation up to the election.
- **situacao** : Whether the candidate was elected.

<br>

## Loading Data

```{r}
readr::read_csv(here::here('data/train_class.csv'),
                progress = FALSE,
                local=readr::locale("br"),
                col_types = cols(ano = col_integer(),
                                 sequencial_candidato = col_character(),
                                 quantidade_doacoes = col_integer(),
                                 quantidade_doadores = col_integer(),
                                 total_receita = col_double(),
                                 media_receita = col_double(),
                                 recursos_de_outros_candidatos.comites = col_double(),
                                 recursos_de_pessoas_fisicas = col_double(),
                                 recursos_de_pessoas_juridicas = col_double(),
                                 recursos_proprios = col_double(),
                                 `recursos_de_partido_politico` = col_double(),
                                 quantidade_despesas = col_integer(),
                                 quantidade_fornecedores = col_integer(),
                                 total_despesa = col_double(),
                                 media_despesa = col_double(),
                                 situacao = col_character(),
                                 .default = col_character())) %>%
  mutate(sequencial_candidato = as.numeric(sequencial_candidato),
         estado_civil = as.factor(estado_civil),
         ocupacao = as.factor(ocupacao),
         situacao = as.factor(situacao),
         partido = as.factor(partido),
         grau = as.factor(grau),
         sexo = as.factor(sexo),
         uf = as.factor(uf)) -> data

data %>%
  glimpse()
```

```{r}
data %>%
  map_df(function(x) sum(is.na(x))) %>%
  gather(feature, num_nulls) %>%
  arrange(desc(num_nulls))
```

## Data Exploration

### Imbalance on class distribution

```{r}
data %>%
  ggplot(aes(situacao)) +
  geom_bar() +
  labs(x="Situation", y="Absolute Frequency")
```

```{r}
data %>%
  group_by(situacao) %>%
  summarise(num = n()) %>%
  ungroup() %>%
  mutate(total = sum(num),
         proportion = num/total)
```

<br>

#### There's a strong imbalance in the class distribution of the dataset with around 13% of the entries in the class "eleito" (elected).

* This imbalance can lead to a bias in the model that will learn to overlook the less frequent classes. Such bias can have a negative impact in the model generalization and its performance.
    + We can restore balance by removing instances from the most frequent class $undersampling$.
    + We can restore balance by adding instances from the most frequent class $oversampling$.

```{r}
data %>% 
  select(-ano,
         -sequencial_candidato,
         -nome) %>%
  select(
    quantidade_doacoes,
    quantidade_doadores,
    total_receita,
    media_receita,
    recursos_de_outros_candidatos.comites,
    recursos_de_pessoas_fisicas,
    recursos_de_pessoas_juridicas,
    recursos_proprios,
    `recursos_de_partido_politico`) %>%
  na.omit() %>%
  ggcorr(palette = "RdBu", label = TRUE,
       hjust = 0.95, label_size = 3,size = 3,
       nbreaks = 5, layout.exp = 5) +
  ggtitle("Correlation plot for employed variables")
```

*  Predictors such as quantidade_doacoes (Number of Donations) and quantidade_doadores (Number of Donators) are highly correlated and therefore redundant. 

```{r}
data %>%
  ggplot(aes(situacao,recursos_proprios)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from personal resources (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_partido_politico)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from political party. (R$)", x="Situation")
```

* Candidates who were elected had overall more revenue form their political party

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_outros_candidatos.comites)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from other candidate’s committees (R$)", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_fisicas)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from individuals (R$)", x="Situation")
```

* Candidates who were elected had overall more revenue form individuals.


```{r}
data %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_juridicas)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Revenue from legal entities (R$)", x="Situation")
```

* Financial support from legal entities (such as companies) seems to have been a game changer on whether a candidate was elected or not.

```{r}
data %>%
  ggplot(aes(situacao,
             quantidade_doacoes)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Number of donations", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             quantidade_doadores)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Number of donators", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             media_receita)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Mean expenditure", x="Situation")
```

```{r}
data %>%
  ggplot(aes(situacao,
             total_receita)) +
  geom_boxplot() + 
  coord_flip() +
  labs(y="Total expenditure", x="Situation")
```

* Who got elected spent a lot more, once again money seems to be the real gatekeeper. 

```{r}
data %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  guides(fill = guide_legend(title = "Sex"))  +
  labs(x="Situation") 
```

* Relatively, we see more men being elected.

```{r}
data %>%
ggplot() +
  geom_mosaic(aes(x = product(grau, situacao),
                   fill=grau)) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  guides(fill = guide_legend(title = "Level of education"))  +
  labs(x="Situation") 
```

* Those with a better level of education seem to have had the upper hand in the elections.  

```{r}
data %>%
ggplot() +
   geom_mosaic(aes(x = product(estado_civil, situacao),
                   fill=estado_civil)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  guides(fill = guide_legend(title = "Marital Status"))  +
  labs(x="Situation") 
```

* Relatively, married people are more likely to be elected in this group.

<br>

## Preparing data 

### Splitting data and Encoding

```{r}
set.seed(107)

data$id <- 1:nrow(data)

data %>% 
  dplyr::sample_frac(.8) -> train

cat("#### Train Shape",
    "\n##### Observations: ",nrow(train),
    "\n##### Variables: ",ncol(train))
```

```{r}
dplyr::anti_join(data, 
                 train, 
                 by = 'id') -> test

cat("#### Test Shape",
    "\n##### Observations: ",nrow(test),
    "\n##### Variables: ",ncol(test))
```

```{r results='asis'}
encoding <- build_encoding(dataSet = data,
                          cols = c("uf","sexo","grau","ocupacao",
                                   "partido","estado_civil"),
                          verbose = F)

train <- one_hot_encoder(dataSet = train,
                          encoding = encoding,
                          drop = TRUE,
                          verbose = F)

cat("#### Train Shape",
    "\n##### Observations: ",nrow(train),
    "\n##### Variables: ",ncol(train))
```

```{r}
test <- one_hot_encoder(dataSet = test,
                          encoding = encoding,
                          drop = TRUE,
                          verbose = F)

cat("#### Data Shape",
    "\n##### Observations: ",nrow(test),
    "\n##### Variables: ",ncol(test))
```

### Near Zero Variance Predictors

```{r}
train %>%
  nearZeroVar(saveMetrics = TRUE) %>%
  tibble::rownames_to_column("variable") %>%
  filter(nzv == T) %>% 
  pull(variable) -> near_zero_vars

train %>% 
    select(-one_of(near_zero_vars)) %>%
    select(-ano,-nome,-id,-sequencial_candidato) -> train

test %>%
    select(-one_of(near_zero_vars)) %>%
    select(-ano,-nome,-id,-sequencial_candidato) -> test


near_zero_vars %>% 
  glimpse() 
```

### Scale and Center

```{r}
train %>%
  preProcess(.,method = c("center","scale")) -> processParams

processParams %>%
  predict(.,train) -> train 

processParams %>% 
  predict(.,test) -> test 

processParams
```

<br>

***

<br>

# Unbalanced dataset

* In this section we shall test the candidate models without applying any sort of undersampling or oversampling to the employed data

<br>

## Logistic Regression 

```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred,
                     y_true = data$obs,
                     positive = lev[1])
  c(F1 = f1_val)
}

F_Measure <- function(expected, predicted, ...) {
  data.frame(expected=expected,
             prediction=predicted) %>%
      mutate(TP = ifelse(expected == "eleito" & 
                         prediction == "eleito",1,0),
             TN = ifelse(expected == "nao_eleito" &
                         prediction == "nao_eleito",1,0),
             FN = ifelse(expected == "eleito" &
                         prediction == "nao_eleito",1,0),
             FP = ifelse(expected == "nao_eleito" &
                         prediction == "eleito",1,0)) -> result
  result  %>%
    summarize(TP = sum(TP),
              TN = sum(TN),
              FP = sum(FP),
              FN = sum(FN)) %>%
    mutate(recall = TP / (TP + FN),
           precision = TP / (TP + FP),
           accuracy = (TP + TN)/(TP + TN + FP + FN),
           f_measure = 2 * (precision * recall) / (precision + recall)) -> result
  
  return(result)
}
```


```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = c(0.001,0.01) )
train %>%
  caret::train(situacao ~ .,
               data= .,
               method = "regLogistic",
               metric = "F1",
               trControl = trainControl(method = "boot",
                                        classProbs = TRUE,
                                        summaryFunction = f1,
                                        savePredictions = "final"),
               tuneGrid = rlGrid) -> model.rl

model.rl
```

```{r}
model.rl %$%
  results %>%
  mutate(cost=as.factor(cost)) %>%
  ggplot(aes(epsilon,F1,
             color=cost)) +
  geom_line() +
  geom_point() +
  labs(y= "F1 (Bootstrap)", x="Tolerance") +
  facet_wrap(. ~ loss, labeller = "label_both") +
  guides(color = guide_legend(title = "Cost")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

* The hyperparameter **Loss** seems to be the particularly meaningful for the Logistic Regression performance in this problem.

```{r}
model.rl %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

* **Total Receita** (Total Revenue), **Total Despesa** (Total Expenditure) and **Recursos de pessoas jurídicas** (Revenue from Legal Entities) are together the three most important features in the model.
* Despite regularization a lot of features were employed, however many were considered less important.

### Quality metric

#### Train and Validation

```{r}
model.rl %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* A decent although uninspiring result.

#### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.rl,.) %>%
  F_Measure(test$situacao,.)
```

* The test result is rather close to the train/validation results, this is a good sign. However, the model results are not particularly promising.

<br>

## K nearest neighbours

```{r}
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

train %>%
  train(situacao ~ .,
        data = .,
        metric = "F1",
        method = "knn",
        na.action = na.omit,
        tuneGrid = neighborsGrid,
        trControl = trainControl(method = "boot",
                                 classProbs = TRUE,
                                 summaryFunction = f1,
                                 savePredictions = "final")) -> model.knn

model.knn
```

```{r}
model.knn %$%
  bestTune %$% 
  k -> bestParameter

model.knn %$%
  results %>%
  ggplot(aes(k,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="#Neighbors",
       y="F1 (Bootstrap)") 
```

```{r}
model.knn %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

* **Total Receita** (Total Revenue), **Total Despesa** (Total Expenditure) and **Recursos de pessoas jurídicas** (Revenue from Legal Entities) are together the three most important features in this model as well.
* The KNN algorithm made use of almost all predictors, this may be a bad sign.

### Quality metric

#### Train and Validation

```{r}
model.knn %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* Here we have worse results than those of the Logistic Regression.

#### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.knn,.) %>%
  F_Measure(test$situacao,.)
```

* We have modest results, but they are consonant with those from train/test.
    + This suggests that the KNN model may be too simple for this particular instance, and bias would be the predominant component in the model error.

<br>

## Decision Tree

```{r}
rpart.grid <- expand.grid(.cp = seq(from=0, to=0.1, by=0.005))

caret::train(x = select(train, -situacao),
             y = train$situacao,
             metric = "F1",
             method = "rpart",
             na.action = na.omit,
             tuneGrid = rpart.grid,
             trControl = trainControl(method = "boot",
                                      classProbs = TRUE,
                                      summaryFunction = f1,
                                      savePredictions = "final")) -> model.tree

model.tree
```

```{r}
model.tree %$%
  bestTune %$% 
  cp -> bestParameter

model.tree %$%
  results %>%
  ggplot(aes(cp,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="Complexity Parameter",
       y="F1 (Bootstrap)") 
```

```{r}
model.tree %$%
  finalModel %>%
  fancyRpartPlot(sub="")
```

* The best performing tree seems way too simple.
    + This suggest an underfitting.

```{r}
model.tree %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,Overall),y = Overall),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

* **Total Receita** (Total Revenue), **Total Despesa** (Total Expenditure) and **Recursos de pessoas jurídicas** (Revenue from Legal Entities) are together the three most important features in this model as well.

### Quality metric

#### Train and Validation

```{r}
model.tree %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* Here we have surprisingly good results considering the simplicity of the tree.

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.tree,.) %>%
  F_Measure(test$situacao,.)
```

* The quality metric for test was bigger than for train/validation
    + This unusual behavior once again hints at the possibility of an underfitting.

<br>

## AdaBoost

```{r}
gbmGrid <- expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                       shrinkage = c(.12, .13),
                       n.trees = c(55, 56, 57, 58, 59, 60),
                       n.minobsinnode = c(2,3))

seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

train %>%
  train(situacao ~ .,
        data = ., 
        method = "gbm", 
        metric = "F1", 
        verbose = FALSE,
        tuneGrid = gbmGrid,
        distribution = "adaboost",
        trControl = trainControl(savePredictions = "final",
                                      summaryFunction = f1,
                                      classProbs = TRUE,
                                      method = "boot",
                                      seeds = seeds)) -> model.gbm
model.gbm
```

```{r}
model.gbm %$%
  results %>%
  ggplot(aes(n.trees,F1, 
             color=as.factor(interaction.depth))) +
  geom_point() +
  geom_line() +
  facet_wrap(. ~ n.minobsinnode+ shrinkage,
             labeller = "label_both") +
  labs(x="# Boosting Iterations",y="F1 (Bootstrap)") +
  guides(color = guide_legend(title = "Max Tree Depth")) 
```

### Train and Validation

```{r}
model.gbm %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* Here we have better results than what we have seen so far.

### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.gbm,.) %>%
  F_Measure(test$situacao,.)
```

* The loss in the quality metric going from train/validation to test was small enough.
  + This suggest that the model did in fact fitted the behavior of the problem and not noise.

<br>

***

<br>

# Applying Oversample (SMOTE)

```{r}
train %>%
  SMOTE(situacao ~ .,
        data = ., 
        perc.over = 200, 
        perc.under=200) -> oversampled

cat("#### Train Shape",
    "\n##### Observations: ",nrow(oversampled),
    "\n##### Variables: ",ncol(oversampled))
```

```{r}
oversampled %>%
  group_by(situacao) %>%
  summarise(num = n()) %>%
  ungroup() %>%
  mutate(total = sum(num),
         proportion = num/total)
```

* BY means of SMOTE we have reduce the class imbalance considerably

<br>

## Logistic Regression with SMOTE 

```{r}
rlGrid <- expand.grid( cost = c(0.02,0.1,2,20,100,200),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = seq(from=0.001,to=0.1, by=0.005) )
oversampled %>%
  caret::train(situacao ~ .,
               data= .,
               method = "regLogistic",
               metric = "F1",
               trControl = trainControl(method = "boot",
                                        classProbs = TRUE,
                                        summaryFunction = f1,
                                        savePredictions = "final"),
               tuneGrid = rlGrid) -> model.rl.smote

model.rl.smote
```

```{r}
model.rl.smote %$%
  results %>%
  mutate(cost=as.factor(cost)) %>%
  ggplot(aes(epsilon,F1,
             color=cost)) +
  geom_line() +
  geom_point() +
  labs(y= "F1 (Bootstrap)", x="Tolerance") +
  facet_wrap(. ~ loss, labeller = "label_both") +
  guides(color = guide_legend(title = "Cost")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

* The hyperparameter **Loss** seems to be the particularly meaningful for the Logistic Regression performance in this problem.

```{r}
model.rl.smote %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

* **Total Receita** (Total Revenue), **Total Despesa** (Total Expenditure) and **Recursos de pessoas jurídicas** (Revenue from Legal Entities) are together the three most important features in this model as well.

### Quality metric

#### Train and Validation

```{r}
model.rl.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* The results in train/validation are a little too optimistic in reason of the imbalance correction and should not be taken seriously. 

* Actual progress shall be assessed in the test stage

#### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.rl.smote,.) %>%
  F_Measure(test$situacao,.)
```

* We can see a substantial improvement compared with our previous attempt using Logistic Regression.

<br>

## K nearest neighbours with SMOTE

```{r}
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

oversampled %>%
  train(situacao ~ .,
        data = .,
        metric = "F1",
        method = "knn",
        na.action = na.omit,
        tuneGrid = neighborsGrid,
        trControl = trainControl(method = "boot",
                                 classProbs = TRUE,
                                 summaryFunction = f1,
                                 savePredictions = "final")) -> model.knn.smote

model.knn.smote
```

```{r}
model.knn.smote %$%
  bestTune %$% 
  k -> bestParameter

model.knn.smote %$%
  results %>%
  ggplot(aes(k,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="#Neighbors",
       y="F1 (Bootstrap)") 
```

* Our cross validation led to a knn with K=1, this may hint at a overfitting.

```{r}
model.knn.smote %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,eleito),y = eleito),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

* **Total Receita** (Total Revenue), **Total Despesa** (Total Expenditure) and **Recursos de pessoas jurídicas** (Revenue from Legal Entities) are together the three most important features in this model as well.

### Quality metric

#### Train and Validation

```{r}
model.knn.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* We have train/validation results incredibly good, in fact too good.
    + This adds up to the hypothesis of a overfitting.

#### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.knn.smote,.) %>%
  F_Measure(test$situacao,.)
```

* And here we have a steep decline in the test results, this is clear evidence that our KNN plus SMOTE overfitted.

<br>

## Decision Tree with SMOTE

```{r}
rpart.grid <- expand.grid(.cp = seq(from=0, to=0.1, by=0.005))

caret::train(x = select(oversampled, -situacao),
             y = oversampled$situacao,
             metric = "F1",
             method = "rpart",
             na.action = na.omit,
             tuneGrid = rpart.grid,
             trControl = trainControl(method = "boot",
                                      classProbs = TRUE,
                                      summaryFunction = f1,
                                      savePredictions = "final")) -> model.tree.smote

model.tree.smote
```

```{r}
model.tree.smote %$%
  bestTune %$% 
  cp -> bestParameter

model.tree.smote %$%
  results %>%
  ggplot(aes(cp,F1)) +
  geom_vline(xintercept = bestParameter,
             color = "red") +
  geom_point(color="#0D98E8") +
  geom_line(color="#0D98E8") +
  labs(x="Complexity Parameter",
       y="F1 (Bootstrap)") 
```

```{r}
model.tree.smote %>%
  varImp() %$%
  importance %>%
  as.data.frame() %>%
  rownames_to_column(var="Feature") %>%
  mutate(Feature = tolower(Feature)) %>%
  ggplot() +
  geom_col(aes(x = reorder(Feature,Overall),y = Overall),
           position = position_dodge(width=0.8),width=0.6) + 
  labs(x="Feature", y="Overall Importance") +
  coord_flip()
```

* **Total Receita** (Total Revenue), **Total Despesa** (Total Expenditure) and **Recursos de pessoas jurídicas** (Revenue from Legal Entities) are together the three most important features in this model as well.

```{r}
model.tree.smote %$%
  finalModel %>%
  fancyRpartPlot(sub="")
```

* Here we have a tree of reasonable depth where the tree most important features have been integrated.

### Quality metric

#### Train and Validation

```{r}
model.tree.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

#### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.tree.smote,.) %>%
  F_Measure(test$situacao,.)
```

* A result similar to that of the Logistic Regression plus SMOTE

<br>

## AdaBoost with SMOTE

```{r}
gbmGrid <- expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                       shrinkage = c(.12, .13),
                       n.trees = c(55, 56, 57, 58, 59, 60),
                       n.minobsinnode = c(2,3))

seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

oversampled %>%
  train(situacao ~ .,
        data = ., 
        method = "gbm", 
        metric = "F1", 
        verbose = FALSE,
        tuneGrid = gbmGrid,
        distribution = "adaboost",
        trControl = trainControl(savePredictions = "final",
                                      summaryFunction = f1,
                                      classProbs = TRUE,
                                      method = "boot",
                                      seeds = seeds)) -> model.gbm.smote
model.gbm.smote
```

```{r}
model.gbm.smote %$%
  results %>%
  ggplot(aes(n.trees,F1, 
             color=as.factor(interaction.depth))) +
  geom_point() +
  geom_line() +
  facet_wrap(. ~ n.minobsinnode+ shrinkage,
             labeller = "label_both") +
  labs(x="# Boosting Iterations",y="F1 (Bootstrap)") +
  guides(color = guide_legend(title = "Max Tree Depth")) 
```

### Quality metric


#### Train and Validation

```{r}
model.gbm.smote %$% 
  pred %>% 
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

* The results in train/validation are a little too optimistic in reason of the imbalance correction and should not be taken seriously. 

* Actual progress shall be assessed in the test stage

#### Test

```{r}
test %>%
  select(-situacao) %>%
  predict(object=model.gbm.smote,.) %>%
  F_Measure(test$situacao,.)
```

* Results similar to those of the Logistic Regression